<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" type="image/png" href="img/Yuulis_icon.png">
    <link rel="stylesheet" href="style.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZM41JSMPWD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-ZM41JSMPWD');
    </script>

    <title>3. 強化学習とは? | 強化学習でAIを避難させてみた</title>
</head>

<body>
    <div class="pageback">
        <div class="pagebody">
            <h1><u>強化学習とは?</u></h1>

            <p><a href="index.html">&gt;&gt;&gt; Topに戻る</a></p>

            <h2 class="heading">機械学習 = 強化学習ではない！</h2>

            <p>「機械学習なら聞いたことある！」という方は多いのではないでしょうか。<b>機械学習(Machine
                    Learning)</b>というのは、<b>人工知能のプログラムが大量のデータを学習し、その学習で得た経験に基づいて識別や予測を行うアルゴリズム(与えられた課題に対しての解決策)を自動で構築する技術全般</b>を指します。実は機械学習には、主に3つの手法が存在します。<b>教師あり学習</b>、<b>教師なし学習</b>、そして<b>強化学習</b>です。
            </p>
            <p><img src="img/MachineLearningDiagram.png" alt="機械学習のグループ分け"></p>
            <p class="centering"><small>出典: <a
                        href="https://mse238blog.stanford.edu/2017/07/choftun/the-building-blocks-of-machine-learning/">Stanford
                        University -The Building Blocks of ML</a></small></p>

            <h3>1. 教師あり学習(Supervised Learning) / 教師なし学習(Unsupervised Learning)</h3>
            <p>教師あり学習は、<b>「入力」と「正しい出力」が結びついた学習データを与え、ある入力があったときに正しい出力を返すアルゴリズムを構築する</b>方法です。電子メールのスパム判定や、機械の故障予測などが例として挙げられます。
            </p>
            <p>それに対して、教師なし学習は、<b>「入力」のみが与えられ、その中に潜在するパターンなどを読み取って出力するアルゴリズムを構築する</b>方法です。コンビニなどの購買データの分析や、画像認識などに使われています。
            </p>

            <h3>2. 強化学習 (Reinforcement Learning)</h3>
            <p>強化学習では、「入力」や「出力」は与えられず、<b>ある「環境」の中で、行動に対して与えられる報酬を最大化するアルゴリズムを構築する</b>方法です。5年前に、囲碁の人工知能「Alpha
                Go」が世界王者に勝利したという大事件が起きました。Alpha Goは強化学習で自分自身との対戦を繰り返し、急成長しました。</p>
            <p>さらに強化学習は、人間の脳の仕組みを再現した「ニューラルネットワーク」を用いた、<b>深層学習(Deep Learning)</b>と組合わせて、<b>深層強化学習(Deep Reinforcement
                    Learning)</b>という手法に応用されています。強化学習の概念自体は機械学習が生まれた当初にありましたが、近年、深層強化学習が登場し技術がより一層発展することとなりました。</p>

            <h2 class="heading">強化学習ライブラリML-Agentsの仕組み</h2>

            <p>ここからは、私が今回の研究に用いた、Unityで強化学習を扱うライブラリ「ML-Agents」の仕組みを簡単に説明します。ここはポスターの内容とは少しずれるので、読み飛ばしていただいても構いません。</p>
            <p>ML-Agentsの実行モードには、<b>学習モード</b>、<b>推論モード</b>、<b>ヒューリスティックモード</b>の3つのモードがあります。</p>

            <h3>学習モード</h3>
            <p>学習モードでは、学習用のスクリプトがUnity環境に接続することでエージェントが学習を開始します。その際の学習アルゴリズムは、⑤ポリシー更新で利用されます。<b>ポリシー</b>はいわゆる<b>エージェントが高い報酬を得られるようにするための戦略</b>のようなもので、これが行動の決定につながります。学習終了時には、学習済みのポリシーがモデルファイルとして出力されます。
            </p>
            <p><img src="img/mlagents_learning.png" alt="学習モード"></p>

            <h3>推論モード</h3>
            <p>推論モードでは、「学習モード」で出力されたモデルファイルをUnity環境に読み込み、エージェントのポリシーとして使用します。<a href="chapter5.html">5.
                    学習結果</a>に掲載している動画の様子が「推論モード」です。このモードでは、学習用のスクリプトは利用せず、エージェントのポリシー更新なども行われません。</p>
            <p><img src="img/mlagents_inference.png" alt="推論モード"></p>

            <h3>ヒューリスティックモード</h3>
            <p>ヒューリスティックモードでは、ポリシーの代わりに人間がエージェントの操作を行います。主に、Unity環境の動作チェックや、人工知能と人間の対戦などの場面で利用されます。このモードでも、学習用のスクリプトは利用せず、やエージェントのポリシー更新なども行われません。
            </p>
            <p><img src="img/mlagnets_heuristic.png" alt="ヒューリスティックモード"></p>

            <p>また、強化学習のサイクルについても説明します。</p>
            <p>まずは強化学習サイクルの単位から。強化学習における、<b>1回分の学習</b>(ゲーム開始からゲームオーバーまで)を<b>エピソード</b>といいます。エージェントはこの1エピソードでもらえる報酬を最大化しようと学習を繰り返します。
            </p>
            <p>一方、学習環境における<b>1フレーム</b>(ゲームなどで60fpsといったフレームレートを聞くと思いますが、あのフレームのこと)を<b>ステップ</b>と言います。エージェントは毎フレームごとに行動していているわけではなく、5フレームごとや10フレームごとおきに行動しています。これは、エージェントの動きをコマ送りのようにして滑らかに見せるためです。もし毎フレームごとに行動したとすると、Unityの場合1フレームは約0.02秒ですから、カクカク震えているような動きになってしまいます。1エピソード内のステップ数は任意に設定することができます。
            </p>

            <p>そこで、1エピソードの強化学習サイクルの流れは、以下のようになります。</p>
            <p><img src="img/mlagents_cycle.png" alt="強化学習のサイクル"></p>
            <h3>①環境観察</h3>
            <p>エージェントが学習環境の情報を受け取ります。情報の受け取り方は、自分の今の座標を数値で直接受け取ったり、周囲にレーザーを発射して検知するものがあるかどうかを調べたりなど、様々な方法があります。</p>
            <h3>②行動決定</h3>
            <p>①で得た情報によって、エージェントの持つポリシーが行動を決定します。初めはランダムな動きを見せますが、正しく学習すると報酬がより多くもらえるような行動をするようになります。</p>
            <h3>③行動実行</h3>
            <p>②でポリシーが決定した行動をエージェントが実行します。</p>
            <h3>④報酬取得</h3>
            <p>エージェントが③で行動した結果に応じて報酬を得ます。この時の報酬の与え方は、人間側が決めてあげる必要があります。</p>
            <h3>⑤ポリシー更新</h3>
            <p>①～④までで、「どんな状態でどんな行動をしたらどのくらい報酬が得られるか」という経験を得ます。その経験に応じてポリシーが更新されます。</p>

            <p class="footer">強化学習についての説明は以上です。</p>
        </div>
    </div>
</body>

</html>